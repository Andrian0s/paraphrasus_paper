{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T01:41:00.343992Z",
     "start_time": "2024-12-13T01:41:00.342198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n"
   ],
   "id": "b28e26311dfce2d6",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T01:41:02.213629Z",
     "start_time": "2024-12-13T01:41:02.211110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "name_mapping = {\n",
    "    'stsbenchmark': 'stsbenchmark-test-sts',\n",
    "    'ms_mrpc': 'ms-mrpc',\n",
    "    'onestop_all': 'onestop_parallel_all_pairs',\n",
    "    'simple_amr': 'amr_true_paraphrases',\n",
    "    'fb_anli_pre_hyp': 'fb-anli-pre-hyp',\n",
    "    'fb_anli_hyp_pre': 'fb-anli-hyp-pre',\n",
    "    'sickr_sts': 'sickr-sts',\n",
    "    'pawsx_test': 'paws-x-test',\n",
    "    'stanfordnlp_snli_pre_hyp': 'stannlp-snli-pre-hyp',\n",
    "    'fb_xnli_pre_hyp': 'fb-xnli-pre-hyp',\n",
    "    'fb_xnli_hyp_pre': 'fb-xnli-hyp-pre',\n",
    "    'stanfordnlp_snli_hyp_pre': 'stannlp-snli-hyp-pre'\n",
    "}\n"
   ],
   "id": "4b0f87ad2031b1c1",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T01:41:02.547557Z",
     "start_time": "2024-12-13T01:41:02.544969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "NO_ID_KEY = \"NOID\"\n",
    "MULTI_LANG_KEY = \"MULTILANG\"\n",
    "ID_KEY = \"id\"\n",
    "id_mapping = {\n",
    "    'stsbenchmark': ID_KEY,\n",
    "    'ms_mrpc': NO_ID_KEY,\n",
    "    'onestop_all': 'OriginalID',\n",
    "    'simple_amr': NO_ID_KEY,\n",
    "    'fb_anli_pre_hyp': ID_KEY,\n",
    "    'fb_anli_hyp_pre': ID_KEY,\n",
    "    'sickr_sts': ID_KEY,\n",
    "    'pawsx_test': MULTI_LANG_KEY,\n",
    "    'stanfordnlp_snli_pre_hyp': ID_KEY,\n",
    "    'fb_xnli_pre_hyp': MULTI_LANG_KEY,\n",
    "    'fb_xnli_hyp_pre': MULTI_LANG_KEY,\n",
    "    'stanfordnlp_snli_hyp_pre': ID_KEY,\n",
    "}\n",
    "lang_key_mapping = {\n",
    "    'pawsx_test': 'language',\n",
    "    'fb_xnli_pre_hyp': 'lang',\n",
    "    'fb_xnli_hyp_pre': 'lang',\n",
    "}\n",
    "\n",
    "def get_original_data(key: str)->pd.DataFrame:\n",
    "    fname = name_mapping[key]+\".tsv\"\n",
    "    path = os.path.join(\"original_reproduction_code/datasets\", fname)\n",
    "    return pd.read_csv(path, sep='\\t')\n"
   ],
   "id": "15ce02451349cfa5",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T01:52:56.039846Z",
     "start_time": "2024-12-13T01:52:54.207767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_json_dict(key: str, dict, path: str) -> None:\n",
    "    fname = name_mapping[key]+\".json\"\n",
    "    path = os.path.join(path, fname)\n",
    "    print(f\"saving {path}\")\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def id_for_record(dataset_key: str, i: int, element: pd.Series):\n",
    "    id_field = id_mapping[dataset_key]\n",
    "    # special cases\n",
    "    if \"sts\" in dataset_key:\n",
    "        id = f\"stsbenchmark-{i}\"\n",
    "    elif \"sick\" in dataset_key:\n",
    "        id = f\"sick-r-{i}\"\n",
    "    elif id_field == NO_ID_KEY:\n",
    "        id = f\"{dataset_key}/{i}\"\n",
    "    elif id_field == MULTI_LANG_KEY:\n",
    "        lang_key = lang_key_mapping[dataset_key]\n",
    "        lang = element[lang_key]\n",
    "        id = f\"{dataset_key}/{lang}-{i}\"\n",
    "    else:\n",
    "        id = element[id_field]\n",
    "\n",
    "    return id\n",
    "\n",
    "def convert_datasets_to_json_templates():\n",
    "    for dataset_key in name_mapping.keys():\n",
    "        records = {}\n",
    "        df = get_original_data(dataset_key)\n",
    "\n",
    "        for i, e in df.iterrows():\n",
    "            s_key = 'sentence'\n",
    "            s1_key = s_key+'1'\n",
    "            s2_key = s_key+'2'\n",
    "            s1 = e[s1_key]\n",
    "            s2 = e[s2_key]\n",
    "            id = id_for_record(dataset_key, i, e)\n",
    "            if id not in records.keys():\n",
    "                records[id] = {}\n",
    "            records[id][s1_key] = s1\n",
    "            records[id][s2_key] = s2\n",
    "\n",
    "        human_annotation_column_name = 'Human Annotation - Consensus'\n",
    "        if human_annotation_column_name in df.columns:\n",
    "            for i, e in df.iterrows():\n",
    "                score = e['score']\n",
    "                c = e[human_annotation_column_name]\n",
    "                label = (c == 1.0)\n",
    "                # only consider this label if score>=4, meaning if this is part of the sts-h dataset.\n",
    "                if score >= 4:\n",
    "                    id = id_for_record(dataset_key, i, e)\n",
    "                    records[id]['label'] = label\n",
    "        elif 'label' in df.columns:\n",
    "            for i, e in df.iterrows():\n",
    "                label = (e['label'] == 1)\n",
    "                id = id_for_record(dataset_key, i, e)\n",
    "                records[id]['label'] = label\n",
    "\n",
    "\n",
    "        if 'score' in df.columns:\n",
    "            for i, e in df.iterrows():\n",
    "                score = e['score']\n",
    "                id = id_for_record(dataset_key, i, e)\n",
    "                records[id]['score'] = score\n",
    "        save_json_dict(dataset_key, records, \"datasets_no_results\")\n",
    "\n",
    "\n",
    "convert_datasets_to_json_templates()"
   ],
   "id": "77f8f19947005ffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving datasets_no_results/stsbenchmark-test-sts.json\n",
      "saving datasets_no_results/ms-mrpc.json\n",
      "saving datasets_no_results/onestop_parallel_all_pairs.json\n",
      "saving datasets_no_results/amr_true_paraphrases.json\n",
      "saving datasets_no_results/fb-anli-pre-hyp.json\n",
      "saving datasets_no_results/fb-anli-hyp-pre.json\n",
      "saving datasets_no_results/sickr-sts.json\n",
      "saving datasets_no_results/paws-x-test.json\n",
      "saving datasets_no_results/stannlp-snli-pre-hyp.json\n",
      "saving datasets_no_results/fb-xnli-pre-hyp.json\n",
      "saving datasets_no_results/fb-xnli-hyp-pre.json\n",
      "saving datasets_no_results/stannlp-snli-hyp-pre.json\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T01:53:01.439023Z",
     "start_time": "2024-12-13T01:53:00.554115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make sure the way we identify a record in a dataset, is correct (id is unique for that dataset only once)\n",
    "for dataset_key in name_mapping.keys():\n",
    "    ids = defaultdict(int)\n",
    "    df = get_original_data(dataset_key)\n",
    "    id_field = id_mapping[dataset_key]\n",
    "\n",
    "    for i, element in df.iterrows():\n",
    "\n",
    "        if id_field == NO_ID_KEY:\n",
    "            id = f\"{dataset_key}/{i}\"\n",
    "        elif id_field == MULTI_LANG_KEY:\n",
    "            lang_key = lang_key_mapping[dataset_key]\n",
    "            lang = element[lang_key]\n",
    "            id = f\"{dataset_key}/{lang}-{i}\"\n",
    "        else:\n",
    "            id = element[id_field]\n",
    "        ids[id] += 1\n",
    "    for id, count in ids.items():\n",
    "        if count > 1:\n",
    "            print(f\"{dataset_key}/{id}: {count}\")"
   ],
   "id": "44edb2333c3a8833",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T01:53:03.123805Z",
     "start_time": "2024-12-13T01:53:03.093379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_unique_column_names(directory_path):\n",
    "    \"\"\"\n",
    "    Finds unique column names across all TSV files in a given directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing TSV files.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of unique column names from all TSV files.\n",
    "    \"\"\"\n",
    "    unique_columns = set()\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Check if the file is a TSV file\n",
    "        if filename.endswith('.tsv'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                # Read the TSV file into a DataFrame\n",
    "                df = pd.read_csv(file_path, sep='\\t', nrows=1)  # Read only the header row for efficiency\n",
    "                # Update the set of unique columns\n",
    "                unique_columns.update(df.columns)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return unique_columns\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"original_reproduction_code/datasets\"  # Replace with the path to your directory\n",
    "unique_columns = find_unique_column_names(directory_path)\n",
    "\n",
    "prefixes = [\n",
    "    # \"CLEAN-\",\n",
    "    \"XLM-RoBERTa-EN-\",\n",
    "    \"LLama3 zero-shot\",\n",
    "    \"LLama3 ICL_4\"\n",
    "]\n",
    "def is_column_of_interest(column_name):\n",
    "    for prefix in prefixes:\n",
    "        if column_name.lower().startswith(prefix.lower()):\n",
    "            return True\n",
    "cols_of_interest = [c for c in unique_columns if is_column_of_interest(c)]\n",
    "\n",
    "print(\"\\n\".join(sorted(list(cols_of_interest))))"
   ],
   "id": "3f91118d220eae7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLama3 ICL_4 (Ex. Same Content)\n",
      "LLama3 ICL_4 (Paraph)\n",
      "LLama3 ICL_4 (Sem Equiv)\n",
      "LLama3 zero-shot (Ex. Same Content)\n",
      "LLama3 zero-shot (Paraph)\n",
      "LLama3 zero-shot (Sem Equiv)\n",
      "XLM-RoBERTa-EN-ALLTHREE-V2-1\n",
      "XLM-RoBERTa-EN-ALLTHREE-V2-2\n",
      "XLM-RoBERTa-EN-ALLTHREE-V2-3\n",
      "XLM-RoBERTa-EN-ALLTHREE-V3-1\n",
      "XLM-RoBERTa-EN-ALLTHREE-V3-2\n",
      "XLM-RoBERTa-EN-ALLTHREE-V3-3\n",
      "XLM-RoBERTa-EN-ALLTHREE-V3-4\n",
      "XLM-RoBERTa-EN-ALLTHREE-V4-1\n",
      "XLM-RoBERTa-EN-ALLTHREE-V4-2\n",
      "XLM-RoBERTa-EN-ALLTHREE-V4-3\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V2-1\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V2-2\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V2-3\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V3-1\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V3-2\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V3-3\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V4-1\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V4-2\n",
      "XLM-RoBERTa-EN-EASYNEG-25-V4-3\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V2-1\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V2-2\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V2-3\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V3-1\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V3-2\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V3-3\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V4-1\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V4-2\n",
      "XLM-RoBERTa-EN-EASYNEG-50-V4-3\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V2-1\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V2-2\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V2-3\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V3-1\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V3-2\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V3-3\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V4-1\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V4-2\n",
      "XLM-RoBERTa-EN-EASYNEG-75-V4-3\n",
      "XLM-RoBERTa-EN-ORIG-V2-1\n",
      "XLM-RoBERTa-EN-ORIG-V2-2\n",
      "XLM-RoBERTa-EN-ORIG-V2-3\n",
      "XLM-RoBERTa-EN-ORIG-V3-1\n",
      "XLM-RoBERTa-EN-ORIG-V3-2\n",
      "XLM-RoBERTa-EN-ORIG-V3-3\n",
      "XLM-RoBERTa-EN-ORIG-V4-2\n",
      "XLM-RoBERTa-EN-ORIG-V4-3\n",
      "XLM-RoBERTa-EN-PARAPH-V2-1\n",
      "XLM-RoBERTa-EN-PARAPH-V2-2\n",
      "XLM-RoBERTa-EN-PARAPH-V2-3\n",
      "XLM-RoBERTa-EN-PARAPH-V3-1\n",
      "XLM-RoBERTa-EN-PARAPH-V3-2\n",
      "XLM-RoBERTa-EN-PARAPH-V3-3\n",
      "XLM-RoBERTa-EN-PARAPH-V4-1\n",
      "XLM-RoBERTa-EN-PARAPH-V4-2\n",
      "XLM-RoBERTa-EN-PARAPH-V4-3\n",
      "XLM-RoBERTa-EN-SAMECONTENT-V2-2\n",
      "XLM-RoBERTa-EN-SAMECONTENT-V2-3\n",
      "XLM-RoBERTa-EN-SAMECONTENT-V3-1\n",
      "XLM-RoBERTa-EN-SAMECONTENT-V3-2\n",
      "XLM-RoBERTa-EN-SAMECONTENT-V3-3\n",
      "XLM-RoBERTa-EN-SAMECONTENT-V4-1\n",
      "XLM-RoBERTa-EN-SAMECONTENT-V4-2\n",
      "XLM-RoBERTa-EN-SAMECONTENT-V4-3\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V2-1\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V2-2\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V2-3\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V3-1\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V3-2\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V3-3\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V4-1\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V4-2\n",
      "XLM-RoBERTa-EN-SEMEQUIV-V4-3\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T01:55:44.225702Z",
     "start_time": "2024-12-13T01:55:36.861908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_paper_json_dict(key: str)->dict:\n",
    "    fname = name_mapping[key]+\".json\"\n",
    "    path = os.path.join(\"benches/paper\", fname)\n",
    "    print(f\"reading {path}\")\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "def add_data_record(dataset_key:str, i:int, paper_element: pd.Series, json_dict):\n",
    "    id = id_for_record(dataset_key, i, paper_element)\n",
    "    if id not in json_dict.keys():\n",
    "        id = f\"{id}\"\n",
    "        if id not in json_dict.keys():\n",
    "            print(f\"ERROR! [id field is {id_field}]{id} is not found in the dataset {dataset_key}. Dict keys: {json_dict.keys()}\")\n",
    "            exit(1)\n",
    "\n",
    "    for col in paper_element.keys():\n",
    "        if col in cols_of_interest:\n",
    "            paper_value = paper_element[col]\n",
    "            if paper_value == 1:\n",
    "                json_dict[id][col] = True\n",
    "            elif paper_value == 0:\n",
    "                json_dict[id][col] = False\n",
    "            else:\n",
    "                print(f\"Unexpected value {paper_value} for {col}: {paper_element}\")\n",
    "\n",
    "for dataset_key in name_mapping.keys():\n",
    "    df = get_original_data(dataset_key)\n",
    "    json_dict = get_paper_json_dict(dataset_key)\n",
    "    for i, element in df.iterrows():\n",
    "        add_data_record(dataset_key, i, element, json_dict)\n",
    "    save_json_dict(dataset_key, json_dict, \"benches/paper\")\n",
    "    # add_data_record(dataset_key, id_mapping[dataset_key], json_dict, json_dict)\n",
    "\n"
   ],
   "id": "a537d329d4e61ab2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading benches/paper/stsbenchmark-test-sts.json\n",
      "saving benches/paper/stsbenchmark-test-sts.json\n",
      "reading benches/paper/ms-mrpc.json\n",
      "saving benches/paper/ms-mrpc.json\n",
      "reading benches/paper/onestop_parallel_all_pairs.json\n",
      "saving benches/paper/onestop_parallel_all_pairs.json\n",
      "reading benches/paper/amr_true_paraphrases.json\n",
      "saving benches/paper/amr_true_paraphrases.json\n",
      "reading benches/paper/fb-anli-pre-hyp.json\n",
      "saving benches/paper/fb-anli-pre-hyp.json\n",
      "reading benches/paper/fb-anli-hyp-pre.json\n",
      "saving benches/paper/fb-anli-hyp-pre.json\n",
      "reading benches/paper/sickr-sts.json\n",
      "saving benches/paper/sickr-sts.json\n",
      "reading benches/paper/paws-x-test.json\n",
      "saving benches/paper/paws-x-test.json\n",
      "reading benches/paper/stannlp-snli-pre-hyp.json\n",
      "saving benches/paper/stannlp-snli-pre-hyp.json\n",
      "reading benches/paper/fb-xnli-pre-hyp.json\n",
      "saving benches/paper/fb-xnli-pre-hyp.json\n",
      "reading benches/paper/fb-xnli-hyp-pre.json\n",
      "saving benches/paper/fb-xnli-hyp-pre.json\n",
      "reading benches/paper/stannlp-snli-hyp-pre.json\n",
      "saving benches/paper/stannlp-snli-hyp-pre.json\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c8b6cd938a2c6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
